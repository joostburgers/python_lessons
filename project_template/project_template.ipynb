{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a623198-174f-44bc-aa35-0c3de16393d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Project Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05be1b3-aad5-4c07-9451-0a352cbc7d98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The following template repeats the essential steps for scraping, tokenizing, and mapping sentiment data. Each major step is further broken down into smaller steps. You can step through most of the code. \n",
    "- You will  have to create your own dataframe names\n",
    "- Customize the filters\n",
    "- Customize the maps\n",
    "\n",
    "#### Note\n",
    "\n",
    "When your individual input is required the following symbol will appear:</br> \n",
    "![](edit-code.png)\n",
    "\n",
    "You should save your results to a `pickle` file after each major step. These points are indicated by the save icon:</br>\n",
    "![](save.png)\n",
    "</br>\n",
    "You will need to import these results into the `project_presentation_template`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d29cb-efdb-47af-8194-1c655d3c9d35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b51b58-f44e-4136-90fb-7e07fd39b1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2 Data Input and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fceaf-0eb8-43b4-893a-edc89bfd04c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell",
     "remove-cell"
    ]
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In this step, you will download a CSV file from Gutenberg, modify it, and save it locally for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410354c-d0ad-42cd-a00c-a6209e7ee3dc",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell",
     "remove-cell"
    ]
   },
   "source": [
    "#### Process Steps:\n",
    "- Load a CSV file from a remote source\n",
    "- Save the file locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e93b30-85e5-42fa-a6e7-2a0032c655a7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell",
     "remove-cell"
    ]
   },
   "source": [
    "### Step-by-Step Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3505e26-76c9-4443-a36f-e1d4c066272d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "#### Step 1: Import the Required Libraries\n",
    "\n",
    "The primary library for handling CSV data in Python is `pandas`. Make sure it’s imported at the beginning of your script.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5f2a2-2f92-4232-bb90-e66bb23679ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02676462-0aae-4fb4-ba9a-ad31fab2d91d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "#### Step 2: Import Pickle File \n",
    "In lesson 2_1 we created a clean pickle file of the catalog, we can simply start there rather than running through all those steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03226c8b-c440-4d6e-bc0c-d92c6d46cd7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pg_catalog_clean=pd.read_pickle('pg_catalog_clean.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d76f05-7766-423b-a48c-ff1c3a84643b",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1594e-00a5-42d8-bbe5-5c7e6c5558b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "![](edit-code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed77fb6-2c68-4625-92a4-088e1384a853",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Step 3: Create a custom dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d911b23-42d1-4a68-95a3-f74eaef89ff0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "##### Overview\n",
    "\n",
    "In lesson 2_2 we learned how to filter dataframes. For this part of the project, you are going to create your own pg_catalog dataframe that includes the corpus of works you want to analyze. Your corpus can be as large as you want, but keep in mind that having a lot of text can significantly increase your processing time. \n",
    "\n",
    "##### Requirements\n",
    "Please follow the following parameters when creating our custom dataframe:\n",
    "\n",
    "1. Unique descriptive dataframe name (i.e. not `df_virginia_history`)\n",
    "\n",
    "2. Corpus should have a logical coherence.\n",
    "\n",
    "3. You should aim for at least 25 texts\n",
    "\n",
    "4. Save the resulting dataframe as a pickle file\n",
    "\n",
    "##### Note\n",
    "Remember to make a deep copy of your dataframe by using the method `.copy()`\n",
    "\n",
    "###### *Example*\n",
    "```python\n",
    "df_virginia_history = df_pg_catalog[\n",
    "    (df_pg_catalog.language == 'en') & \n",
    "    (df_pg_catalog.type == 'Text') \n",
    "].copy()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f548a-6ba3-4478-b464-c800e1ca0af1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Save the dataframe to a pickle file: \n",
    "\n",
    "```python\n",
    "df_virginia_history.to_pickle('custom_file.pickle')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d88e2-7949-4e17-85a0-b881c37c1bd2",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6024e-8008-47ad-8952-1d93ef456f29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3 Scraping Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df652a50-5080-4257-8b9a-bfebd08224c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "In the previous section you were meant to create a custom dataframe. Here you are going to scrape that data frame. The scraping function assumes that your first column is called `text_id`. If that is not the case then something went wrong with the filtering process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec5f3ac-2636-45b5-b84e-005bbdd81caa",
   "metadata": {},
   "source": [
    "#### Load `gutenberg_scraper`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73535c-1915-45db-86c0-2dec95e19951",
   "metadata": {},
   "source": [
    "Since you only need to perform one function `fetch_text_data()` all of the other logic has been tucked away in a `.py` file. You can load the python file and function in like any other import as long as the file is present in your root directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab7a12-d4f1-4838-b866-05b5dd4e1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenberg_scraper import fetch_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e9168-0ab9-475c-9a6b-ab72ebb82143",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This line is here to let you walk through the code. It takes a random sample from the Project Gutenberg Catalog and runs it through all the steps.\n",
    "#YOUR_DATAFRAME = pg_catalog_clean.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5fe59-80c2-402b-8a04-0b03d971dcef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fetch_text_data(YOUR_DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2659202-82ef-47a5-93e3-69ff6b84c3e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Export Pickle 1\n",
    "\n",
    "![](save.png)\n",
    "\n",
    "Export the file as a `pickle` file for presentation.\n",
    "\n",
    "```python\n",
    "YOUR_DATAFRAME.to_pickle('YOUR_DATAFRAME_TEXTS.pickle')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b28cc7-6edc-474f-a040-c3bc3195c599",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023ebcb-8e19-42f8-8697-9dd2c601bc8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![](edit-code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00866dad-11dd-4503-89da-618a7bf8cbbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4 Clean DataFrame for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd300887-ac66-40d0-89e3-e5cd9e70f019",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To prepare our data for analysis we will: \n",
    "\n",
    "- Split it into sentences\n",
    "- Clean the individual sentences\n",
    "- Drop unnecessary data\n",
    "\n",
    "For the code below you will have to replace `YOUR_DATAFRAME` with the name of your dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efeff86-0969-4373-820c-0979e952d5c3",
   "metadata": {},
   "source": [
    "### Import `NLTK`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b555d9-b0be-42b9-a393-f4caf4eedf0b",
   "metadata": {},
   "source": [
    "We can use NLTK for some basic preprocessing.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "import re\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c025a-74d4-47b1-9348-3454000b3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ddb570-bbea-4e67-920a-b2416528b8cd",
   "metadata": {},
   "source": [
    "### Step 1: Tokenize Text into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4625bbb-dc5b-4876-9e5d-034592781008",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explodes the DataFrame so that each row corresponds to a single sentence\n",
    "YOUR_DATAFRAME = YOUR_DATAFRAME.assign(\n",
    "    sentences=YOUR_DATAFRAME['text_data'].apply(nltk.sent_tokenize)\n",
    ").explode('sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927422b1-dec5-4605-980d-9e793f7e088b",
   "metadata": {},
   "source": [
    "### Step 2: Remove the 'text_data' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1172f-38a5-424b-b7a0-9253f3aa4ce8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = YOUR_DATAFRAME.drop(columns='text_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2f7bd-0c9d-4a63-b967-f0ebee9eb176",
   "metadata": {},
   "source": [
    "### Step 3: Define a Cleaning Function for Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18ac15-75ad-4dca-a43c-84ddd832e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # 1. Remove text inside square brackets\n",
    "    sentence = re.sub(r'\\[.*?\\]', '', sentence)\n",
    "    # 2. Remove unwanted punctuation but retain sentence-ending punctuation\n",
    "    sentence = re.sub(r'[^\\w\\s,.!?\\'\"‘’“”`]', '', sentence)\n",
    "    # 3. Remove newline and carriage return characters, and underscores\n",
    "    sentence = sentence.replace('\\n', ' ').replace('\\r', ' ').replace('_', '')\n",
    "    # 4. Return an empty string for all-uppercase sentences (likely headers or TOC entries)\n",
    "    return '' if sentence.isupper() else sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea310d-6b6e-4a26-a8e6-8a755d47ae26",
   "metadata": {},
   "source": [
    "### Step 4: Apply Cleaning and Remove Empty Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b711517-7800-400c-a71f-0638f405f07b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the cleaning function, then filter out any sentences that are empty strings\n",
    "YOUR_DATAFRAME['cleaned_sentences'] = YOUR_DATAFRAME['sentences'].apply(clean_sentence)\n",
    "YOUR_DATAFRAME = YOUR_DATAFRAME[YOUR_DATAFRAME['sentences'] != '']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd44120-c579-4e11-bb24-ab9123d2f249",
   "metadata": {},
   "source": [
    "### Step 5: Reset Index for the Cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9b05c-6c30-4d7e-b1f4-5f185f3dbd27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = YOUR_DATAFRAME.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11506913-a96b-4e7d-b0fb-00dfde26e71b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 6: (OPTIONAL) Save deep copy of dataframe and pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f024a95-3af8-43df-9e35-45c237a53f1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEW_DATAFRAME_NAME = YOUR_DATAFRAME.copy()\n",
    "NEW_DATAFRAME_NAME.to_pickle('NEW_DATAFRAME_NAME.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8df70-eeb6-4c60-accf-aa05e80343e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046f5d8-92db-49ef-a1d3-5bb282a14b2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 5 Perform Initial Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15fae02-24dd-41b1-999d-af44280842e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "Since the geoparsing process is quite intense, we can actually reduce our processing overhead a bit by eliminating those sentences that likely don't have toponyms. We can do so by first running a pass with the lightweight `en_core_web_sm` `spacy` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea9d11-153f-429b-b48b-25486ec59588",
   "metadata": {},
   "source": [
    "### Load Spacy\n",
    "We are going to load spacy and the small library at the same time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067a82d-bb79-4a02-b0df-07f9dc72c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73517785-c924-4a20-8877-507c5ec933e8",
   "metadata": {},
   "source": [
    "#### Load Functions into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f6e52-bad8-49c5-b91f-5c82e911be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract GPE (Geopolitical Entities) from a batch of docs\n",
    "def extract_gpe_from_docs(docs):\n",
    "    return [[ent.text for ent in doc.ents if ent.label_ == 'GPE'] or None for doc in docs]\n",
    "\n",
    "# Use nlp.pipe() for faster batch processing with multiple cores\n",
    "def process_sentences_in_batches(sentences, batch_size=50, n_process=-1):\n",
    "    # Process sentences using nlp.pipe with batch processing and multi-processing\n",
    "    gpe_results = []\n",
    "    for doc in tqdm(nlp.pipe(sentences, batch_size=batch_size, n_process=n_process), total=len(sentences)):\n",
    "        gpes = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        gpe_results.append(gpes if gpes else None)\n",
    "    return gpe_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9771e-7377-4882-8457-f0ddc8f59558",
   "metadata": {},
   "source": [
    "#### Process your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08dcf1-9d1d-43f7-b539-a7aa37ac344c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME['toponyms'] = process_sentences_in_batches(YOUR_DATAFRAME['cleaned_sentences'])\n",
    "(YOUR_DATAFRAME['cleaned_sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0351bf-c9fc-4e1c-bf2a-beb5262de8dc",
   "metadata": {},
   "source": [
    "### Clean up the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69815d-19e0-4b64-a8b2-7370db2cdb1b",
   "metadata": {},
   "source": [
    "As we saw in the most intense part of the extraction process in lesson_5 we want to reduce the number of sentences being processed to lower the computation time. We can do two things at this stage. \n",
    " 1. Eliminate unnecessary columns\n",
    " 2. Eliminate all sentences for which there is no result\n",
    " 3. Eliminate all sentences with very few results. Your group can decide on the threshold, but suffice to say that all toponyms with a count of 1 won't be relevant. You can adjust this number as you fine-tune your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bae1d6-4a9e-4a33-8b91-48d2577ef6c7",
   "metadata": {},
   "source": [
    "#### Eliminate Unncessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840929a-d2c2-4ff7-98f3-c80cb45ecbc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = YOUR_DATAFRAME.drop(columns=['language', 'issued', 'type', 'locc', 'bookshelves', 'second_author']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a5363-117f-48dd-83b2-b0efef6f4052",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Eliminate `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d43f9c-5420-4047-bf85-883c28fdcdde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = YOUR_DATAFRAME[YOUR_DATAFRAME.toponyms.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6336eb2-31b0-40bb-8b83-8ba2cced60ee",
   "metadata": {},
   "source": [
    "To eliminate the some of the complicated processing the function below adds a count column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd0206-4ede-4375-82c1-863ed583b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_toponym_count(df, toponym_col='toponyms', sentence_col='cleaned_sentences'):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame to count toponyms and aggregate back to sentences, keeping all original columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing toponyms and sentences.\n",
    "        toponym_col (str): Column containing the toponyms as lists.\n",
    "        sentence_col (str): Column containing the cleaned sentences.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame grouped by sentences with a list of toponyms, their counts, and all original columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Explode the 'toponyms' column\n",
    "    exploded_df = df.explode(toponym_col)\n",
    "    \n",
    "    # Step 2: Group by 'toponyms' to count occurrences and add 'nltk_toponym_count' column\n",
    "    toponym_counts = exploded_df.groupby(toponym_col).size().reset_index(name='nltk_toponym_count')\n",
    "    \n",
    "    # Step 3: Merge the counts back to the exploded DataFrame\n",
    "    exploded_df = exploded_df.merge(toponym_counts, on=toponym_col, how='left')\n",
    "    \n",
    "    # Step 4: Group by 'cleaned_sentences' and aggregate all columns\n",
    "    # Use 'first' to retain the first non-null value for each original column, and 'list' for the toponym_col\n",
    "    aggregation_dict = {col: 'first' for col in df.columns if col not in [sentence_col, toponym_col]}\n",
    "    aggregation_dict[toponym_col] = lambda x: list(x)  # Aggregate toponyms into lists\n",
    "    aggregation_dict['nltk_toponym_count'] = 'first'   # Take the first count (all counts are the same within groups)\n",
    "    \n",
    "    result_df = exploded_df.groupby(sentence_col).agg(aggregation_dict).reset_index()\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75678045-3147-4add-9b78-7c5124d3b0bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = add_toponym_count(YOUR_DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe87915-0312-4ccd-bdf3-50c2923d1468",
   "metadata": {},
   "source": [
    "![](edit-code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc819547-0582-442b-81ab-c03bc375f8b3",
   "metadata": {},
   "source": [
    "#### Filter out low toponym counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfff84-e907-4612-b001-514ded6af753",
   "metadata": {},
   "source": [
    "Your dataframe now has the new variable `nltk_toponym_count`. You can filter out low count results to get fewer sentences. You can get a data frame for all cleaned sentences where the nltk_toponym_count is **greater** than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d100f-67fe-4e05-bd89-d50d7129b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove toponyms with a low count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ba98a-e839-47c4-a1c6-3c0a39f42570",
   "metadata": {},
   "source": [
    "![](save.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853cf963-687c-421d-b7af-090495d6b35d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### (Optional) Save pickle of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2034ff-73b5-4ba9-a09f-44c985d0298c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#YOUR_DATAFRAME_TOPONYM.to_pickle('YOUR_DATAFRAME_TOPONYM.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5420e68-e9c7-4305-90b4-7c99b17c7851",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9a9e3-5175-46ac-8a80-acaf6f4961b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 6 Geoparsing (Deep Scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911c805-4185-4d35-aa32-75c504d67e17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "Since the deep scan for toponyms will likely reduce the size of the dataframe again, we can backload the sentiment analysis as the last step to ensure we don't process data unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771fac4-63cf-48f4-a8a4-75f12db32639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoparser import Geoparser\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde5075-2eeb-4117-9a9a-53e9bf9bf4dd",
   "metadata": {},
   "source": [
    "Because there are some compatibility issues with the `geoparser` package, there are pesky warnings that pop-up. These do not affect the output, but they are annoying. The line below filters these out of the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db235c6c-b309-4a77-a655-09b8154eac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978acb25-34b1-4903-b3e1-ac5ab017245f",
   "metadata": {},
   "source": [
    "### Load Geoparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3c57f-4c2b-444c-9b55-8ed504cb54ea",
   "metadata": {},
   "source": [
    "To use Geoparser, instantiate an object of the Geoparser class with optional specifications for the spaCy model, transformer model, and gazetteer. By default, the library uses an accuracy-optimised configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c5860-4964-4ccd-b224-62401445a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = Geoparser(spacy_model='en_core_web_trf', transformer_model='dguzh/geo-all-distilroberta-v1', gazetteer='geonames')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b05bd9-bbd9-47b9-b6b4-7da886cc9b8a",
   "metadata": {},
   "source": [
    "Load in the `geoparse_column` function to simplify the toponym recognition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ef385-d77d-406d-8707-18959d7a458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoparse_column(df):\n",
    "    sentences = df['cleaned_sentences'].tolist()  # Convert column to list\n",
    "    docs = geo.parse(sentences, feature_filter=['A', 'P'])  # Run geo.parse on the entire list\n",
    "\n",
    "    # Initialize lists to store the extracted fields\n",
    "    places, latitudes, longitudes, feature_names = [], [], [], []\n",
    "\n",
    "    # Iterate through the results and extract toponyms and their locations\n",
    "    for doc in docs:\n",
    "        doc_places = []\n",
    "        doc_latitudes = []\n",
    "        doc_longitudes = []\n",
    "        doc_feature_names = []\n",
    "\n",
    "        for toponym in doc.toponyms:\n",
    "            if toponym.location:\n",
    "                doc_places.append(toponym.location.get('name'))\n",
    "                doc_latitudes.append(toponym.location.get('latitude'))\n",
    "                doc_longitudes.append(toponym.location.get('longitude'))\n",
    "                doc_feature_names.append(toponym.location.get('feature_name'))\n",
    "            else:\n",
    "                doc_places.append(None)\n",
    "                doc_latitudes.append(None)\n",
    "                doc_longitudes.append(None)\n",
    "                doc_feature_names.append(None)\n",
    "\n",
    "        # Append the extracted data for the document\n",
    "        places.append(doc_places)\n",
    "        latitudes.append(doc_latitudes)\n",
    "        longitudes.append(doc_longitudes)\n",
    "        feature_names.append(doc_feature_names)\n",
    "\n",
    "    # Assign the extracted data to the DataFrame as new columns\n",
    "    df['place'] = places\n",
    "    df['latitude'] = latitudes\n",
    "    df['longitude'] = longitudes\n",
    "    df['feature_name'] = feature_names\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375790a9-d5cb-4184-bd31-b656766caff2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoparse_column(YOUR_DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe8b5a-d2c8-497b-97bb-2facb72f9080",
   "metadata": {},
   "source": [
    "![](save.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23545418-afd0-45ee-9c46-8386f2055305",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Export Pickle 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67b1cc-0d10-406f-9348-6bd879ebd2cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As the geoparsing process takes a long time, you should store it right after the result. You will also import these results for your `project_presentation_template`\n",
    "\n",
    "```python\n",
    "YOUR_DATAFRAME.to_pickle('YOUR_DATAFRAME_PLACES.pickle')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0a2a7-d001-493a-a7d5-89c75d30f66d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Clean up the resulting dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f0fec-3c82-4598-a95f-9747cddad4d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As with the previous instance of toponym resolution, there will be some rows that do not contain relevant information. This will slow down the sentiment analysis. \n",
    "1. Eliminate empty results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e6728-e3e5-4fa9-ab95-668334df78d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = YOUR_DATAFRAME[YOUR_DATAFRAME['place'].str.len() != 0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad92253-d754-4d6f-907f-9bb33b494184",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d144b0d-0e89-48ee-b0b1-e12f0a40657c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 7 Run Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde948a-95fb-4b0a-8c62-ec416373be5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "We will now implement the sentiment analysis on the remaining sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c96396-cfa4-4bbb-abcc-48c4cd62dbf7",
   "metadata": {},
   "source": [
    "Read step through and read all the prerequisites into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58a60f-d5af-4a97-9a06-256dad43a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d95f9-1902-415a-8aba-1087cdaf019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RoBERTa. There will probably be a warning. You can ignore this.\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7433bf-aa8d-444e-bb1b-808f8991f4cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate RoBERTa sentiment scores\n",
    "def polarity_scores_roberta(text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate RoBERTa sentiment scores for a given text.\n",
    "    \n",
    "    Args:\n",
    "    - text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with sentiment scores for negative, neutral, and positive sentiment\n",
    "    \"\"\"\n",
    "    # Tokenize and truncate to max length (512 tokens)\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text, \n",
    "        max_length=512, \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Get model output and convert to probabilities\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    return {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42fc29-c461-4985-9c99-0b58a33557a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to attach sentiment analysis to a specific column in the dataframe\n",
    "def add_sentiment_to_column(\n",
    "    df: pd.DataFrame, column_name: str, num_rows: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds RoBERTa sentiment analysis to a specified column in a dataframe.\n",
    "    \n",
    "    Args:\n",
    "    - df: The dataframe to process\n",
    "    - column_name: The name of the column containing the text to analyze\n",
    "    - num_rows: The number of rows to process (default: 500)\n",
    "    \n",
    "    Returns:\n",
    "    - df: A dataframe with added sentiment analysis columns\n",
    "    \"\"\"\n",
    "        # If num_rows is specified, limit the dataframe, otherwise process all rows\n",
    "    if num_rows:\n",
    "        df_subset = df.head(num_rows).reset_index(drop=True)\n",
    "    else:\n",
    "        df_subset = df.reset_index(drop=True)  # Process all rows and reset the index\n",
    "    \n",
    "    # Function to process each row and add sentiment analysis\n",
    "    def process_row(text: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            return polarity_scores_roberta(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {text}. Error: {e}\")\n",
    "            return {'roberta_neg': None, 'roberta_neu': None, 'roberta_pos': None}\n",
    "    \n",
    "    # Apply the RoBERTa sentiment analysis to each row\n",
    "    tqdm.pandas(desc=\"Processing Sentiment Analysis\")\n",
    "    sentiment_scores = df_subset[column_name].progress_apply(process_row)\n",
    "    \n",
    "    # Convert the resulting list of dictionaries into a DataFrame and concatenate it with the original subset\n",
    "    sentiment_df = pd.DataFrame(sentiment_scores.tolist())\n",
    "    df_subset = pd.concat([df_subset, sentiment_df], axis=1)\n",
    "    \n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bde43d-3bf6-47c1-a086-24741d3d5905",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = add_sentiment_to_column(YOUR_DATAFRAME, 'cleaned_sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b37d33-4a25-4119-98ce-e0ecf1257d51",
   "metadata": {},
   "source": [
    "### Create an aggregate score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376df567-2746-4d34-960d-136aa299f622",
   "metadata": {},
   "source": [
    "Since the roberta score is positive, negative, and neutral, we will have to consolidate it into one easier to understand score. We will take the difference between positive and negative, and multiply it by the percentage of neutral. This way if a score is very neutral it will even out the difference between positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f676bc0-24b1-4d86-8e76-2e9ef2724ac2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the compound score and add it as a new column 'roberta_compound'\n",
    "YOUR_DATAFRAME['roberta_compound'] = (\n",
    "    YOUR_DATAFRAME['roberta_pos'] - YOUR_DATAFRAME['roberta_neg']\n",
    ") * (1 - YOUR_DATAFRAME['roberta_neu'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e790fa0-818a-411b-930d-9c648abda9b3",
   "metadata": {},
   "source": [
    "### Explode, filter, and aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38be33-12c6-4fe4-9381-1e8e57dff252",
   "metadata": {},
   "source": [
    "At the moment, there are places and sentiments, but since some of the sentences contain multiple places these need to be unnested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fd80d-7e5e-45b4-83da-bd43e83e0772",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = YOUR_DATAFRAME.explode(['place', 'latitude', 'longitude', 'feature_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf4a52-17aa-4343-ae30-0c8c00f44806",
   "metadata": {},
   "source": [
    "Remove empty values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad73bf-743b-41ae-9388-39d2eea818d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = YOUR_DATAFRAME[YOUR_DATAFRAME.place.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98092d4f-0b12-49a1-9857-76d186a4e66e",
   "metadata": {},
   "source": [
    "Aggregate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ee69a-d384-4954-9b7b-d484fd6b168a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_DATAFRAME = YOUR_DATAFRAME.groupby('place').agg(\n",
    "    location_count=('place', 'size'),  # Count occurrences of each location\n",
    "    latitude=('latitude', 'first'),    # Take the first latitude (you can also use 'mean')\n",
    "    longitude=('longitude', 'first'),  # Take the first longitude (or 'mean')\n",
    "    location=('feature_name','first'),\n",
    "    avg_roberta_pos=('roberta_pos', 'mean'),  # Average of roberta_pos\n",
    "    avg_roberta_neu=('roberta_neu', 'mean'),  # Average of roberta_neu\n",
    "    avg_roberta_neg=('roberta_neg', 'mean'), # Average of roberta_neg\n",
    "    avg_roberta_compound =('roberta_compound','mean')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c3ae8-d468-4274-a3a2-b2358897d75a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Create Histogram of Count Values (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931faf67-1546-4bd3-bffe-51b0b4ac5f47",
   "metadata": {},
   "source": [
    "To get a sense of how the data is distributed and to decide which data to include, you can create a histogram of the `location_count` column fairly easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f693c6-22f4-412a-98e7-a92dd0629375",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# You might need to install matplotlib with \n",
    "# pip install matplotlib\n",
    "\n",
    "YOUR_DATAFRAME.location_count.plot.hist(bins=10, alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccdba55-ed07-4c5f-b50b-6478a06f63de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Generally, the data will be very left skewed. You might want to filter out some of the lower values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb9a7f-2b79-4541-b598-fb0ded693f3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Filter out low counts\n",
    "\n",
    "As very low counts will not show up on the map anyway, filter them out here. No code has been provided, but the procedure is essentially the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef527d-3d4d-48ac-916b-3e8da942f104",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92368a-55db-45a1-8041-2e8e380dd4f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Bucket Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b12403-bd56-4358-a974-5c5ae4c917f0",
   "metadata": {},
   "source": [
    "As we saw in lesson_5, the distribution of the data is tricky. We can solve this by bucketing it along the lines of Jenks Natural Breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5bf278-7259-411b-af49-f2795836ff52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mapclassify as mc #you may get an error. If so install mapclassify with pip install mapclassify\n",
    "\n",
    "jenks_breaks = mc.NaturalBreaks(y=YOUR_DATAFRAME['location_count'], k=5)\n",
    "YOUR_DATAFRAME.loc[:,'location_count_bucket'] = jenks_breaks.find_bin(YOUR_DATAFRAME['location_count'])+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db5d13-7875-4399-a34f-e7171d3bb790",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Export Pickle 3\n",
    "\n",
    "This is the final export of the file for the `project_presentation_template`\n",
    "\n",
    "```python\n",
    "YOUR_DATAFRAME.to_pickle('YOUR_DATAFRAME_SENTIMENTS.pickle')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5a5b2-b26c-4a8b-afae-a256aef28174",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9a09fc2-1bc4-45cc-b9cf-010bf9de1bb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa370e7-11ea-4724-8740-fb28b1689715",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Map your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3a5bc-326f-421c-9076-9ddecb303e4e",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This is the core of the project. Use the stub below to map your data and then customize the map. I have deliberately set some of the values very poorly to encourage you to work on your own map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4807f6a-0f82-4fdc-9157-9f1c0a590348",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    YOUR_DATAFRAME,  #put your dataframe here\n",
    "    lat=\"latitude\",               # Latitude column\n",
    "    lon=\"longitude\",              # Longitude column\n",
    "    size=\"location_count_bucket\",        # Bubble size based on location count\n",
    "    color=\"avg_roberta_compound\",      # Color based on sentiment score\n",
    "    color_continuous_scale=px.colors.cyclical.Twilight[::-1],  # Use Twilight scale (blue to red)\n",
    "    size_max=30,                  # Maximum size of the bubbles\n",
    "    center={\"lat\": 48, \"lon\": 2},\n",
    "    zoom=6                       # Adjust zoom level for better visibility\n",
    ")\n",
    "\n",
    "# Update the layout to use the default map style (which doesn't need a token)\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"open-street-map\",  # No token needed for this style\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}  # Remove margins for a cleaner view\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea273a46-697a-4829-b795-f81f36b179a8",
   "metadata": {},
   "source": [
    "Happy mapping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5995c-e3a7-427b-826b-d80cc1984d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
