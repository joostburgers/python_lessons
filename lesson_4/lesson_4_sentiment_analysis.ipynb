{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c62347c9-2b6e-400e-9e7f-097aee0784fa",
   "metadata": {},
   "source": [
    "# Lesson 4: Sentiment Analysis on Toponym Sentences\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lesson will cover two sentiment analysis methods:\n",
    "- Using the **NLTK** library's VADER sentiment analysis tool.\n",
    "- Using **Hugging Face's RoBERTa** model for sentiment analysis.\n",
    "\n",
    "We will compare how these two tools perform on sentences containing toponyms extracted from the `virginia_toponyms_pickle` file, and we will store the results in a **Pandas DataFrame** for further analysis. The key goal is to understand how different tools analyze sentiment, identify their limitations, and explore why their outputs might differ.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926d4f5-6b52-4ecc-9405-3266bc99d330",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset\n",
    "\n",
    "We will begin by loading the data containing the sentences with toponyms into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd49fbd-ea3d-437d-8da9-b4ff0d4b1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0058505d-aeb6-4e17-be32-feace41783b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_toponyms = pd.read_pickle('df_virginia_toponyms.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa73f9-be61-4923-b7bf-5d98d0244bb6",
   "metadata": {},
   "source": [
    "### 1.1 More clean up...yes that's most of the work!\n",
    "\n",
    "In the previous lesson, we discovered that toponyms for UPPER CASE sentences were mostly garbage. Let's drop those to reduce processing overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70506553-4947-4e26-b35e-e5f37ed16403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_toponyms = df_virginia_toponyms[~df_virginia_toponyms.cleaned_sentences.str.isupper()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a2475-7842-4485-83d5-ab315947360b",
   "metadata": {},
   "source": [
    "#### 1.1.1 Reset the index\n",
    "\n",
    "Since we extracted all of the sentences, the index (left most columns) got messed up. This wasn't an issue for the toponyms, but will cause problems for the sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a6641c-61a0-4fdd-8bfa-5ee8ea9876d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_toponyms =df_virginia_toponyms.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015858a6-2f24-45d3-9e45-f7bc5dd61fb8",
   "metadata": {},
   "source": [
    "#### 1.1.2 Drop Unnecessary Columns\n",
    "\n",
    "As the dataframe keeps getting wider and wider, we'll want to drop some unnecessary columns just so the view is manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "976d4d89-ce5f-4e3a-a929-da10887f8e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>last_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>birth</th>\n",
       "      <th>death</th>\n",
       "      <th>cleaned_sentences</th>\n",
       "      <th>toponyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29097</th>\n",
       "      <td>44229</td>\n",
       "      <td>The Birth of the Nation, Jamestown, 1607</td>\n",
       "      <td>Virginia -- History -- Colonial period, ca. 16...</td>\n",
       "      <td>Pryor</td>\n",
       "      <td>Sara Agnes Rice</td>\n",
       "      <td>1830</td>\n",
       "      <td>1912</td>\n",
       "      <td>Smith  was sent overland to invite the Emperor...</td>\n",
       "      <td>[Smith]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14106</th>\n",
       "      <td>32507</td>\n",
       "      <td>The Planters of Colonial Virginia</td>\n",
       "      <td>Virginia -- History -- Colonial period, ca. 16...</td>\n",
       "      <td>Wertenbaker</td>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>1879</td>\n",
       "      <td>1966</td>\n",
       "      <td>Why, it was asked, should  Englishmen be forc...</td>\n",
       "      <td>[America]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>27117</td>\n",
       "      <td>Tobacco in Colonial Virginia \"The Sovereign Re...</td>\n",
       "      <td>Tobacco -- Virginia -- History; Virginia -- Hi...</td>\n",
       "      <td>Herndon</td>\n",
       "      <td>G. Melvin</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>In 1771 there were  rumors that at least one h...</td>\n",
       "      <td>[Virginia]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_id                                              title  \\\n",
       "29097    44229           The Birth of the Nation, Jamestown, 1607   \n",
       "14106    32507                  The Planters of Colonial Virginia   \n",
       "4584     27117  Tobacco in Colonial Virginia \"The Sovereign Re...   \n",
       "\n",
       "                                                subjects    last_name  \\\n",
       "29097  Virginia -- History -- Colonial period, ca. 16...        Pryor   \n",
       "14106  Virginia -- History -- Colonial period, ca. 16...  Wertenbaker   \n",
       "4584   Tobacco -- Virginia -- History; Virginia -- Hi...      Herndon   \n",
       "\n",
       "             first_name  birth  death  \\\n",
       "29097   Sara Agnes Rice   1830   1912   \n",
       "14106  Thomas Jefferson   1879   1966   \n",
       "4584          G. Melvin   <NA>   <NA>   \n",
       "\n",
       "                                       cleaned_sentences    toponyms  \n",
       "29097  Smith  was sent overland to invite the Emperor...     [Smith]  \n",
       "14106   Why, it was asked, should  Englishmen be forc...   [America]  \n",
       "4584   In 1771 there were  rumors that at least one h...  [Virginia]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_virginia_toponyms_compact = df_virginia_toponyms.drop(columns=['language', 'issued', 'type', 'locc', 'bookshelves', 'second_author']).copy()\n",
    "df_virginia_toponyms_compact.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8309329-c761-4cc3-8b64-0dd40341239d",
   "metadata": {},
   "source": [
    "#### 1.1.3 Set Panda width to max\n",
    "\n",
    "Some of the sentences are quite long and to see them all on the screen we will need to change the width of the columns to max.\n",
    "\n",
    "```python\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "```\n",
    "When we are done we can set this back to a more reasonable number by replacing `None` with an integer.\n",
    "\n",
    "```python\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd82be0d-14fc-4853-b74c-9d895ee9151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b44da-13fe-4a65-adb2-db633029f4b2",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis with NLTK (VADER)\n",
    "\n",
    "### 2.1 Overview\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. It was largely trained on twitter, and really only looks at sentiment-per-word. This makes it relatively speedy, but there are some issues with this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077901fe-4cb8-4d57-afce-d2ba68dafa25",
   "metadata": {},
   "source": [
    "### 2.2 Loading VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d71ddcf-1fda-47af-9deb-b3e7ecdf3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd788e9-98cd-4541-b7c3-c92d8ae13228",
   "metadata": {},
   "source": [
    "You will only need to download the lexicon once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b1fdd31-dbd4-492f-a43f-cf5c34762474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\joost\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download the 'vader_lexicon'\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33b69344-fabb-4b55-95da-94eb47fe3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c100465-3f73-4ead-89bd-d93b97d961e8",
   "metadata": {},
   "source": [
    "### 2.3 Using `sia.polarity_scores()`\n",
    "\n",
    "The sentiment analyzer works by applying the VADER model to any text passed into the function `sia.polarity_scores()`. It will then generate a list of scores for that particular phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8bfba7-a673-47bb-9552-f5a20e987090",
   "metadata": {},
   "source": [
    "#### 2.3.1 Good Vibes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "373830d9-d816-4845-9cc6-4e4d2bd40302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.471, 'pos': 0.529, 'compound': 0.6696}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('JMU is the best university!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4f244-53b6-4d80-b2d5-87175b80f548",
   "metadata": {},
   "source": [
    "#### 2.3.2 Bad Vibes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "980ce729-96cd-4ba1-8740-fbfa3415d6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.423, 'neu': 0.577, 'pos': 0.0, 'compound': -0.5661}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('UVA is not the best university!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6934e-ec42-43e3-8527-9115d64bd1c2",
   "metadata": {},
   "source": [
    "### 2.4 Critical Thinking Challenge\n",
    "\n",
    "For the next activity, you are going to try to push the limits of the tokenizer. For each challenge, think of a sentence that will get the scores you want, even if those scores don't make sense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0dee4c-69a4-4ec4-ba24-4ba88800cb6e",
   "metadata": {},
   "source": [
    "#### 2.4.1 Most Goodest Vibes\n",
    "\n",
    "Try to create a sentence with a compound polarity score of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12424bb7-2536-45e6-aafc-d10069ae98c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550828ab-6f6c-4fe2-80ba-7364f238c709",
   "metadata": {},
   "source": [
    "#### 2.4.2 Most Baddest Vibes\n",
    "\n",
    "Try to create a sentence with a compound polarity score of 1.0, but keep it pg-13!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c05efc1-3da8-4af3-afa3-6db635afe0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8636f86e-0798-4391-bc58-6ab42bb3b1bf",
   "metadata": {},
   "source": [
    "#### 2.4.3 Most Strangest Vibes\n",
    "\n",
    "Try to create a sentence with either a positive or negative compound score, but that means the exact opposite of what it says."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bffc1153-2061-4628-bf88-d15913b978fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffcd9da-fa21-4a48-9ef3-d7866412036b",
   "metadata": {},
   "source": [
    "### 2.5 Run VADER on all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ac47d03-8dbe-4d08-85dd-bf90a76d12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on each sentence and store the compound score in the DataFrame\n",
    "df_virginia_toponyms_compact['nltk_sentiment'] = df_virginia_toponyms_compact['cleaned_sentences'].apply(lambda x: sia.polarity_scores(x)['compound'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e02aa-f6b6-45e0-a977-58088c969a6d",
   "metadata": {},
   "source": [
    "See result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cc82d15-1b07-4a8e-a48f-ae9c55e7abbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_sentences</th>\n",
       "      <th>nltk_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39161</th>\n",
       "      <td>Enemies accused him of profiting by the maladministration of his  officials, and he himself confessed in a rather cynical letter to Lord  Arlington that, while advancing years had taken away his ambition, they  had left him covetous.</td>\n",
       "      <td>-0.6597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>Oade.    A thing of so great vent and vse amongst English Diers, which cannot  bee yeelded sufficiently in our owne countrey for spare of ground may  bee planted in Virginia, there being ground enough.</td>\n",
       "      <td>0.7384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45299</th>\n",
       "      <td>Mr. Rubsamen  told me that lead ore is found on New River and the Greenbrier, copper  on the Roanoke Dan, and iron everywhere about, particularly in  Buckingham County.</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26192</th>\n",
       "      <td>All seemed in a reverie,  dreaming a long sweet dream of the past, and entering into the grief  of the sisters, who lived afterward for many years in a pleasant home  on a pleasant street in Richmond, with warm friends to serve them, yet  their tears never ceased to flow at the mention of Mount Erin.</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32279</th>\n",
       "      <td>From the Ohio River to  the sea, from North Carolina to the Pennsylvania line, the people of the  commonwealth were stirred by the fervor of the campaign and the  magnitude of the issues upon which they were called to pass.</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                   cleaned_sentences  \\\n",
       "39161                                                                      Enemies accused him of profiting by the maladministration of his  officials, and he himself confessed in a rather cynical letter to Lord  Arlington that, while advancing years had taken away his ambition, they  had left him covetous.   \n",
       "1233                                                                                                       Oade.    A thing of so great vent and vse amongst English Diers, which cannot  bee yeelded sufficiently in our owne countrey for spare of ground may  bee planted in Virginia, there being ground enough.   \n",
       "45299                                                                                                                                       Mr. Rubsamen  told me that lead ore is found on New River and the Greenbrier, copper  on the Roanoke Dan, and iron everywhere about, particularly in  Buckingham County.   \n",
       "26192  All seemed in a reverie,  dreaming a long sweet dream of the past, and entering into the grief  of the sisters, who lived afterward for many years in a pleasant home  on a pleasant street in Richmond, with warm friends to serve them, yet  their tears never ceased to flow at the mention of Mount Erin.   \n",
       "32279                                                                                From the Ohio River to  the sea, from North Carolina to the Pennsylvania line, the people of the  commonwealth were stirred by the fervor of the campaign and the  magnitude of the issues upon which they were called to pass.   \n",
       "\n",
       "       nltk_sentiment  \n",
       "39161         -0.6597  \n",
       "1233           0.7384  \n",
       "45299          0.0000  \n",
       "26192          0.8885  \n",
       "32279          0.0000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_virginia_toponyms_compact[['cleaned_sentences','nltk_sentiment']].sample(5, random_state=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218d691-34b5-406a-b9ca-b3336ce5e7db",
   "metadata": {},
   "source": [
    "#### 2.6 Evaluate the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3abac-5d07-4d33-a9fe-82a910700301",
   "metadata": {},
   "source": [
    "The compound score ranges from -1 to 1. When a passage is very negative it gets a -1 and when it is possitive it gets a 1. Read through the passages above and try to figure out why these passages received the sentiments they did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f49fe-aa6a-41a5-9cf8-252885bd4ed1",
   "metadata": {},
   "source": [
    "### 2.7 Critical Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad9937-0e6d-49c8-a791-6e8ab77bf088",
   "metadata": {},
   "source": [
    "How effective is the VADER tokenizer in dealing with sentiments in historical manuscripts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa209252-c37d-4477-9b5f-afcad19528ef",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis with Hugging Face (RoBERTa)\n",
    "\n",
    "RoBERTa (Robustly Optimized BERT Pretraining Approach) is a transformer-based model that has been fine-tuned for sentiment analysis tasks. We will use Hugging Face's `transformers` library to analyze the sentiment of the toponym-containing sentences. This model is available on a site called [Hugging Face](https://huggingface.co/). Check out the sentiment models [here](https://huggingface.co/models?sort=trending&search=sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b6439-b060-45ab-960e-c91a5313d346",
   "metadata": {},
   "source": [
    "### 3.1 Prepping your system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cbbc7-41ff-47d5-8051-1a0e7f9b74da",
   "metadata": {},
   "source": [
    "You will need to insall yet more libraries. \n",
    "\n",
    "Open up a new terminal window in Juypter and type the following commands:\n",
    "\n",
    "- `pip install transformers`\n",
    "- `pip install torch`\n",
    "- `pip install scipy`\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede7f00-b3b8-4475-96c0-3c4ecf7688bb",
   "metadata": {},
   "source": [
    "### 3.1.1 Possible Warning\n",
    "\n",
    "When running the import:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "```\n",
    "\n",
    "You might getting a warning message telling you to upgrade Juypter Lab and Ipywidgets. If that is the case use the command:\n",
    "\n",
    "- `conda update jupyterlab`\n",
    "- `conda install -c conda-forge ipywidgets`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7be418-b9c5-47ae-a2ff-c5e04b20a425",
   "metadata": {},
   "source": [
    "## 3.2 Load Functions into memory\n",
    "\n",
    "Getting Roberta to code the sentiments is a fairly common procedure. There is a great in-depth video [here](https://www.youtube.com/watch?v=QpzMWQvxXWk). I have adapted and updated the code for newer versions of Python. The only thing you need to do is to load the functions into memory.\n",
    "\n",
    "Step through the code blocks below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35f09a69-7c2a-446d-83a2-3d5bc456cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f88dce02-6d29-433a-a161-20ca1a0a3d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joost\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize RoBERTa. There will probably be a warning. You can ignore this.\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de4c8595-4a03-46c4-b805-57c6c634db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RoBERTa sentiment scores\n",
    "\n",
    "def polarity_scores_roberta(text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate RoBERTa sentiment scores for a given text.\n",
    "    \n",
    "    Args:\n",
    "    - text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with sentiment scores for negative, neutral, and positive sentiment\n",
    "    \"\"\"\n",
    "    # Tokenize and truncate to max length (512 tokens)\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text, \n",
    "        max_length=512, \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Get model output and convert to probabilities\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    return {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a151c93a-5555-4d92-85fb-509bf7e3cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to attach sentiment analysis to a specific column in the dataframe\n",
    "def add_sentiment_to_column(\n",
    "    df: pd.DataFrame, column_name: str, num_rows: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds RoBERTa sentiment analysis to a specified column in a dataframe.\n",
    "    \n",
    "    Args:\n",
    "    - df: The dataframe to process\n",
    "    - column_name: The name of the column containing the text to analyze\n",
    "    - num_rows: The number of rows to process (default: 500)\n",
    "    \n",
    "    Returns:\n",
    "    - df: A dataframe with added sentiment analysis columns\n",
    "    \"\"\"\n",
    "        # If num_rows is specified, limit the dataframe, otherwise process all rows\n",
    "    if num_rows:\n",
    "        df_subset = df.head(num_rows).reset_index(drop=True)\n",
    "    else:\n",
    "        df_subset = df.reset_index(drop=True)  # Process all rows and reset the index\n",
    "    \n",
    "    # Function to process each row and add sentiment analysis\n",
    "    def process_row(text: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            return polarity_scores_roberta(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {text}. Error: {e}\")\n",
    "            return {'roberta_neg': None, 'roberta_neu': None, 'roberta_pos': None}\n",
    "    \n",
    "    # Apply the RoBERTa sentiment analysis to each row\n",
    "    tqdm.pandas(desc=\"Processing Sentiment Analysis\")\n",
    "    sentiment_scores = df_subset[column_name].progress_apply(process_row)\n",
    "    \n",
    "    # Convert the resulting list of dictionaries into a DataFrame and concatenate it with the original subset\n",
    "    sentiment_df = pd.DataFrame(sentiment_scores.tolist())\n",
    "    df_subset = pd.concat([df_subset, sentiment_df], axis=1)\n",
    "    \n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d91cd3-8d42-4d9c-a844-0ce171396e95",
   "metadata": {},
   "source": [
    "### 3.2 A Very Simple Explanation\n",
    "\n",
    "The code blocks above are quite complex, but they essentially do one thing: add columns with sentiment scores to a dataframe that contains sentences. The function is fairly straightforward and has three possible parameters: \n",
    "\n",
    "- `dataframe` - The dataframe where you want to perform the function. In our case, `df_virginia_toponyms_compact`\n",
    "- `column` - The column name where the sentences are stored\n",
    "- `num_rows=` - (Optional) Integer value of the number of rows you want to process. Since this is very processor intensive. It makes sense to be able to just grab a sample. Leaving this blank will process every row.\n",
    "\n",
    "```python\n",
    "df_virginia_toponym_sentiment_sample = add_sentiment_to_column(df_virginia_toponyms_compact, 'cleaned_sentences', num_rows=1000)\n",
    "```\n",
    "With that explanation in mind, what does the above line of code do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3423a758-c624-4a5a-8797-7559bbadf2f9",
   "metadata": {},
   "source": [
    "### 3.3 Get a Sample Sentiment Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df58631d-e51d-470e-9658-7ecf72dcabd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentiment Analysis: 100%|███████████████████████████████████████████████| 1000/1000 [00:56<00:00, 17.73it/s]\n"
     ]
    }
   ],
   "source": [
    "df_virginia_toponym_sentiment_sample = add_sentiment_to_column(df_virginia_toponyms_compact, 'cleaned_sentences', num_rows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7c41902-a651-4558-855a-fb68aa129400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_toponym_sentiment_sample.to_pickle('df_virginia_toponym_sentiment_sample.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1832469-583a-4293-9152-189488dfd6c7",
   "metadata": {},
   "source": [
    "#### Evaluate the Sample\n",
    "\n",
    "If you could not get the tokenizer to work, you can get the result by running this line of code:\n",
    "\n",
    "```python\n",
    "df_virginia_toponym_sentiment_sample = pd.read_pickle('df_virginia_toponym_sentiment_sample')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80abc459-cb1a-43e3-b37d-f4bd06fad7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_sentences</th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Captain Nathaniel Butler, who had  once been Governor of the Somers Islands and had now returned to England  by way of Virginia, published in London \"The Unmasked Face of Our Colony  in Virginia\", containing a savage attack upon every item of Virginian  administration.</td>\n",
       "      <td>0.478677</td>\n",
       "      <td>0.498520</td>\n",
       "      <td>0.022803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>Blair sailed back to  Virginia with the charter of the college, some money, a plan for the  main building drawn by Christopher Wren, and for himself the office of  President.</td>\n",
       "      <td>0.022498</td>\n",
       "      <td>0.928171</td>\n",
       "      <td>0.049331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>Baltimore was a reflective man, a dreamer in the good sense of the term,  and religiously minded.</td>\n",
       "      <td>0.019395</td>\n",
       "      <td>0.408428</td>\n",
       "      <td>0.572177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>But Rembrandt was not born in Massachusetts  people hardly ever do know where to be born until it is too late.</td>\n",
       "      <td>0.428928</td>\n",
       "      <td>0.527906</td>\n",
       "      <td>0.043166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Incontinently Smith was seized, dragged to a great stone lying before  Powhatan, forced down and bound.</td>\n",
       "      <td>0.500344</td>\n",
       "      <td>0.486323</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                 cleaned_sentences  \\\n",
       "530  Captain Nathaniel Butler, who had  once been Governor of the Somers Islands and had now returned to England  by way of Virginia, published in London \"The Unmasked Face of Our Colony  in Virginia\", containing a savage attack upon every item of Virginian  administration.   \n",
       "926                                                                                                 Blair sailed back to  Virginia with the charter of the college, some money, a plan for the  main building drawn by Christopher Wren, and for himself the office of  President.   \n",
       "586                                                                                                                                                                              Baltimore was a reflective man, a dreamer in the good sense of the term,  and religiously minded.   \n",
       "25                                                                                                                                                                  But Rembrandt was not born in Massachusetts  people hardly ever do know where to be born until it is too late.   \n",
       "332                                                                                                                                                                        Incontinently Smith was seized, dragged to a great stone lying before  Powhatan, forced down and bound.   \n",
       "\n",
       "     roberta_neg  roberta_neu  roberta_pos  \n",
       "530     0.478677     0.498520     0.022803  \n",
       "926     0.022498     0.928171     0.049331  \n",
       "586     0.019395     0.408428     0.572177  \n",
       "25      0.428928     0.527906     0.043166  \n",
       "332     0.500344     0.486323     0.013333  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the results for the first few rows\n",
    "df_virginia_toponym_sentiment_sample[['cleaned_sentences', 'roberta_neg', 'roberta_neu', 'roberta_pos']].sample(5, random_state=47)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6790208-e7ec-469a-9e1e-149b738950fa",
   "metadata": {},
   "source": [
    "#### 3.3.1 Critical Question\n",
    "\n",
    "1. How did the tokenizer do?\n",
    "2. Where would you dispute the sentiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd0547-ce94-41a7-9db9-f22e13206c27",
   "metadata": {},
   "source": [
    "#### 3.3.2 Critical Activity\n",
    "\n",
    "1. Cycle through the samples by changing `random_state=` to a different integer.\n",
    "2. Look through the sentences\n",
    "3. Identify a sentence where the language model does particularly well or poorly.\n",
    "4. If you were not able to run the tokenizer. Load in the sample pickle file below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22f578a9-3a89-4758-b10d-c012cf1ff9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_toponym_sentiment_sample = pd.read_pickle('df_virginia_toponym_sentiment_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e100e7a-5256-4cf2-b336-7d6060a5e851",
   "metadata": {},
   "source": [
    "## 4. Creating the entire dataset\n",
    "\n",
    "This process will take a very long time. I will create this data set for you, but if you ever want to do it on your own. The line of code is below. Simply remove the hashtag to uncomment it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87451dc1-bb0f-471b-bebe-ccbc4c36cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_virginia_toponym_sentiment_full = add_sentiment_to_column(df_virginia_toponyms_compact, 'cleaned_sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9abdd391-f671-4432-a95a-6fa3b70e7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_virginia_toponym_sentiment_full.to_pickle('df_virginia_toponym_sentiment_full.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d83e5a51-9aa0-4964-9eb4-82f2ee308ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_toponym_sentiment_full = pd.read_pickle('df_virginia_toponym_sentiment_full.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "edfb2da2-7b1d-4399-9f2b-fc11c1e1c209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>last_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>birth</th>\n",
       "      <th>death</th>\n",
       "      <th>cleaned_sentences</th>\n",
       "      <th>toponyms</th>\n",
       "      <th>nltk_sentiment</th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14360</th>\n",
       "      <td>32507</td>\n",
       "      <td>The Planters of Colonial Virginia</td>\n",
       "      <td>Virginia -- History -- Colonial period, ca. 1600-1775; Slavery -- Virginia; Virginia -- Economic conditions</td>\n",
       "      <td>Wertenbaker</td>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>1879</td>\n",
       "      <td>1966</td>\n",
       "      <td>If confined to England alone, only a fraction  of the output could be consumed and disaster was certain.</td>\n",
       "      <td>[England]</td>\n",
       "      <td>-0.6124</td>\n",
       "      <td>0.692587</td>\n",
       "      <td>0.286966</td>\n",
       "      <td>0.020447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32351</th>\n",
       "      <td>46026</td>\n",
       "      <td>Virginia's Attitude Toward Slavery and Secession</td>\n",
       "      <td>United States -- Politics and government -- 1861-1865; Slavery -- Virginia; Virginia -- Politics and government -- 1861-1865</td>\n",
       "      <td>Munford</td>\n",
       "      <td>Beverley B. (Beverley Bland)</td>\n",
       "      <td>1856</td>\n",
       "      <td>1910</td>\n",
       "      <td>\"    The Governors of Kentucky, Missouri, Arkansas, Tennessee and North  Carolina returned like answers to the requisitions of the Federal  authorities for troops.</td>\n",
       "      <td>[Missouri, Arkansas, Tennessee, North  Carolina]</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.075799</td>\n",
       "      <td>0.871962</td>\n",
       "      <td>0.052239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7834</th>\n",
       "      <td>29055</td>\n",
       "      <td>The Present State of Virginia</td>\n",
       "      <td>Indians of North America -- Virginia -- Early works to 1800; Slavery -- Virginia -- Early works to 1800; African Americans -- Virginia -- Early works to 1800; Virginia -- Description and travel -- Early works to 1800</td>\n",
       "      <td>Jones</td>\n",
       "      <td>Hugh</td>\n",
       "      <td>1669</td>\n",
       "      <td>1760</td>\n",
       "      <td>Near this is a large Octogon Tower, which is the Magazine or  Repository of Arms and Ammunition, landing far from any House except  James Town CourtHouse for the Town is half in James Town County,  and half in York County.</td>\n",
       "      <td>[York County]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.073226</td>\n",
       "      <td>0.883940</td>\n",
       "      <td>0.042834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23821</th>\n",
       "      <td>39148</td>\n",
       "      <td>How Justice Grew: Virginia Counties, An Abstract of Their Formation</td>\n",
       "      <td>Virginia -- History; Counties -- Virginia -- History; Virginia -- History, Local</td>\n",
       "      <td>Hiden</td>\n",
       "      <td>Martha W. (Martha Woodroof)</td>\n",
       "      <td>1883</td>\n",
       "      <td>1959</td>\n",
       "      <td>Wood and Harrison are also West Virginia counties.</td>\n",
       "      <td>[West Virginia]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.069118</td>\n",
       "      <td>0.890772</td>\n",
       "      <td>0.040110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45413</th>\n",
       "      <td>63221</td>\n",
       "      <td>Travels in Virginia in Revolutionary Times</td>\n",
       "      <td>Virginia -- Description and travel</td>\n",
       "      <td>Morrison</td>\n",
       "      <td>Alfred J. (Alfred James)</td>\n",
       "      <td>1876</td>\n",
       "      <td>1923</td>\n",
       "      <td>Taking a road, however, as nearly as I could guess, in a  direct line from the river up the country, at the end of an hour I came  upon a narrow road, which led to a large old brick house, somewhat  similar to those I had met with on the Maryland shore.</td>\n",
       "      <td>[Maryland]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.023312</td>\n",
       "      <td>0.814797</td>\n",
       "      <td>0.161891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_id  \\\n",
       "14360    32507   \n",
       "32351    46026   \n",
       "7834     29055   \n",
       "23821    39148   \n",
       "45413    63221   \n",
       "\n",
       "                                                                     title  \\\n",
       "14360                                    The Planters of Colonial Virginia   \n",
       "32351                     Virginia's Attitude Toward Slavery and Secession   \n",
       "7834                                         The Present State of Virginia   \n",
       "23821  How Justice Grew: Virginia Counties, An Abstract of Their Formation   \n",
       "45413                           Travels in Virginia in Revolutionary Times   \n",
       "\n",
       "                                                                                                                                                                                                                       subjects  \\\n",
       "14360                                                                                                               Virginia -- History -- Colonial period, ca. 1600-1775; Slavery -- Virginia; Virginia -- Economic conditions   \n",
       "32351                                                                                              United States -- Politics and government -- 1861-1865; Slavery -- Virginia; Virginia -- Politics and government -- 1861-1865   \n",
       "7834   Indians of North America -- Virginia -- Early works to 1800; Slavery -- Virginia -- Early works to 1800; African Americans -- Virginia -- Early works to 1800; Virginia -- Description and travel -- Early works to 1800   \n",
       "23821                                                                                                                                          Virginia -- History; Counties -- Virginia -- History; Virginia -- History, Local   \n",
       "45413                                                                                                                                                                                        Virginia -- Description and travel   \n",
       "\n",
       "         last_name                    first_name  birth  death  \\\n",
       "14360  Wertenbaker              Thomas Jefferson   1879   1966   \n",
       "32351      Munford  Beverley B. (Beverley Bland)   1856   1910   \n",
       "7834         Jones                          Hugh   1669   1760   \n",
       "23821        Hiden   Martha W. (Martha Woodroof)   1883   1959   \n",
       "45413     Morrison      Alfred J. (Alfred James)   1876   1923   \n",
       "\n",
       "                                                                                                                                                                                                                                                   cleaned_sentences  \\\n",
       "14360                                                                                                                                                       If confined to England alone, only a fraction  of the output could be consumed and disaster was certain.   \n",
       "32351                                                                                            \"    The Governors of Kentucky, Missouri, Arkansas, Tennessee and North  Carolina returned like answers to the requisitions of the Federal  authorities for troops.   \n",
       "7834                                  Near this is a large Octogon Tower, which is the Magazine or  Repository of Arms and Ammunition, landing far from any House except  James Town CourtHouse for the Town is half in James Town County,  and half in York County.   \n",
       "23821                                                                                                                                                                                                             Wood and Harrison are also West Virginia counties.   \n",
       "45413  Taking a road, however, as nearly as I could guess, in a  direct line from the river up the country, at the end of an hour I came  upon a narrow road, which led to a large old brick house, somewhat  similar to those I had met with on the Maryland shore.   \n",
       "\n",
       "                                               toponyms  nltk_sentiment  \\\n",
       "14360                                         [England]         -0.6124   \n",
       "32351  [Missouri, Arkansas, Tennessee, North  Carolina]          0.3612   \n",
       "7834                                      [York County]          0.0000   \n",
       "23821                                   [West Virginia]          0.0000   \n",
       "45413                                        [Maryland]          0.0000   \n",
       "\n",
       "       roberta_neg  roberta_neu  roberta_pos  \n",
       "14360     0.692587     0.286966     0.020447  \n",
       "32351     0.075799     0.871962     0.052239  \n",
       "7834      0.073226     0.883940     0.042834  \n",
       "23821     0.069118     0.890772     0.040110  \n",
       "45413     0.023312     0.814797     0.161891  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_virginia_toponym_sentiment_full.sample(5, random_state = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a65921-dfa8-4757-87a6-1a8b50bf82c1",
   "metadata": {},
   "source": [
    "### 4.1 Check peformance\n",
    "\n",
    "We can check the performance of both tokenizers by looking up \"edge cases\" where one gives a negative evaluation and the other positive. \n",
    "\n",
    "#### 4.1.2\n",
    "How would we go about this? I have stubbed out some of the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "445d19eb-ab4b-4029-ac05-73e4a5e5a009",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'float' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:362\u001b[0m, in \u001b[0;36mna_logical_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;66;03m# For exposition, write:\u001b[39;00m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;66;03m#  yarr = isinstance(y, np.ndarray)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# Then Cases where this goes through without raising include:\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m#  (xint or xbool) and (yint or bool)\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(x, y)\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'bitwise_and' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_virginia_toponym_sentiment_full[(df_virginia_toponym_sentiment_full\u001b[38;5;241m.\u001b[39mnltk_sentiment )\u001b[38;5;241m&\u001b[39m\n\u001b[0;32m      2\u001b[0m                                     (df_virginia_toponym_sentiment_full\u001b[38;5;241m.\u001b[39mroberta_pos )]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:70\u001b[0m, in \u001b[0;36mOpsMixin.__and__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__and__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__and__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logical_method(other, operator\u001b[38;5;241m.\u001b[39mand_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6130\u001b[0m, in \u001b[0;36mSeries._logical_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6127\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6128\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 6130\u001b[0m res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mlogical_op(lvalues, rvalues, op)\n\u001b[0;32m   6131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:454\u001b[0m, in \u001b[0;36mlogical_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;66;03m# i.e. scalar\u001b[39;00m\n\u001b[0;32m    452\u001b[0m     is_other_int_dtype \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mis_integer(rvalues)\n\u001b[1;32m--> 454\u001b[0m res_values \u001b[38;5;241m=\u001b[39m na_logical_op(lvalues, rvalues, op)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;66;03m# For int vs int `^`, `|`, `&` are bitwise operators and return\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;66;03m#   integer dtypes.  Otherwise these are boolean ops\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (left\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_other_int_dtype):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:369\u001b[0m, in \u001b[0;36mna_logical_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    367\u001b[0m     x \u001b[38;5;241m=\u001b[39m ensure_object(x)\n\u001b[0;32m    368\u001b[0m     y \u001b[38;5;241m=\u001b[39m ensure_object(y)\n\u001b[1;32m--> 369\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_binop(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# let null fall thru\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_scalar(y)\n",
      "File \u001b[1;32mops.pyx:252\u001b[0m, in \u001b[0;36mpandas._libs.ops.vec_binop\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mops.pyx:245\u001b[0m, in \u001b[0;36mpandas._libs.ops.vec_binop\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'float' and 'float'"
     ]
    }
   ],
   "source": [
    "df_virginia_toponym_sentiment_full[(df_virginia_toponym_sentiment_full.nltk_sentiment )&\n",
    "                                    (df_virginia_toponym_sentiment_full.roberta_pos )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8befbb-90ae-462f-8682-d19f665352c2",
   "metadata": {},
   "source": [
    "## 5. Analyzing the Differences\n",
    "\n",
    "### Differences Between NLTK and RoBERTa:\n",
    "1. **NLTK (VADER)**:\n",
    "    - Uses a lexicon-based approach.\n",
    "    - Performs well on short social media-style texts, but may not capture the full context in longer, more complex sentences.\n",
    "\n",
    "2. **RoBERTa**:\n",
    "    - Uses a transformer-based deep learning model, which can better understand context.\n",
    "    - However, RoBERTa can sometimes be biased towards its training data (in this case, Twitter-based sentiments).\n",
    "\n",
    "### Why Might These Differences Occur?\n",
    "- **Context Understanding**: RoBERTa uses a much more advanced neural network model, allowing it to grasp nuances better than NLTK.\n",
    "- **Lexicon Limitations**: NLTK relies on predefined dictionaries of words, which means it may miss certain contextual clues or interpret complex sentences inaccurately.\n",
    "\n",
    "### Limitations:\n",
    "- **RoBERTa**: While more accurate in many cases, it might be overfitted to specific domains (e.g., Twitter), which could skew its results on historical or formal texts.\n",
    "- **NLTK**: Fast and simple, but its lexicon-based approach might not always provide a detailed or accurate sentiment analysis in nuanced contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28751957-e41f-4128-9d00-a1f28c23fbff",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "By comparing NLTK’s VADER and Hugging Face's RoBERTa, we can see that different sentiment analysis tools offer different strengths. NLTK’s rule-based system is fast and straightforward but can miss complex sentiment cues. RoBERTa, being a transformer model, performs better on context-heavy sentences but can sometimes be biased by its training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72565818-da13-4535-8758-7d45d42c4c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
