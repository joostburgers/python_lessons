{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e1bde0-6884-482e-86f9-428755b47fdc",
   "metadata": {},
   "source": [
    "# Lesson 5: Geoparsing and Sentiment Mapping in Python\n",
    "\n",
    "**üéØ Learning Objectives:**\n",
    "- Extract geographic locations from text using advanced geoparser\n",
    "- Combine location data with sentiment analysis\n",
    "- Create interactive maps to visualize sentiment by location\n",
    "- Apply data science techniques to literary analysis\n",
    "\n",
    "**üìã Prerequisites:**\n",
    "- Complete `lesson_5_0_installation_setup.ipynb` first\n",
    "- Data from previous lessons (sentiment analysis results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10701836-5eff-4b1b-8ee7-24f3d0859333",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lesson, we'll take text data about Virginia's history and:\n",
    "\n",
    "1. **Extract Locations**: Use a sophisticated geoparser to find and resolve geographic references\n",
    "2. **Combine with Sentiment**: Link location mentions to emotional sentiment in the text\n",
    "3. **Visualize on Maps**: Create interactive maps showing sentiment patterns across different places\n",
    "\n",
    "This process helps us understand how different locations are portrayed in historical texts - whether they're mentioned positively, negatively, or neutrally.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb7c37-80eb-4ecf-91da-66d098d8825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "try:\n",
    "    from geoparser import Geoparser\n",
    "    from tqdm.notebook import tqdm\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    import mapclassify as mc\n",
    "    import warnings\n",
    "    \n",
    "    # Suppress warnings for cleaner output\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    print(\"‚úÖ All libraries imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing library: {e}\")\n",
    "    print(\"Please run the installation notebook first: lesson_5_0_installation_setup.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e758d-0e34-4bb2-8ccc-cead9da5ebaf",
   "metadata": {},
   "source": [
    "## Part 1: Initialize the Geoparser System\n",
    "\n",
    "The geoparser is a sophisticated tool that can identify place names in text and resolve them to actual geographic coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6ef4b-2414-4ed6-b360-692b44b061b5",
   "metadata": {},
   "source": [
    "### Step 1.1: Initialize the Geoparser\n",
    "\n",
    "We'll create a geoparser with optimized settings for accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801c355-cc54-477b-8fe7-29f6e6b33de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Initializing geoparser... (this may take a minute)\")\n",
    "    geo = Geoparser(\n",
    "        spacy_model='en_core_web_trf',                    # Advanced language model\n",
    "        transformer_model='dguzh/geo-all-distilroberta-v1', # Geographic transformer\n",
    "        gazetteer='geonames'                              # Geographic database\n",
    "    )\n",
    "    print(\"‚úÖ Geoparser initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing geoparser: {e}\")\n",
    "    print(\"Make sure you ran the installation notebook first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f1ce5-1995-4dac-ac62-9073d34017ef",
   "metadata": {},
   "source": [
    "**What these parameters do:**\n",
    "- `spacy_model`: Advanced language processing for accurate text understanding\n",
    "- `transformer_model`: Specialized AI model trained to recognize geographic references  \n",
    "- `gazetteer`: Database containing millions of place names and their coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a23989-93da-46c4-abe5-2907d5c1aa28",
   "metadata": {},
   "source": [
    "### Step 1.2: Test the Geoparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f23571-3818-4d81-b63f-ed13fdd4096d",
   "metadata": {},
   "source": [
    "Let's test the geoparser with some sample sentences. Try changing the text below to include places you know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1b7b7-3399-462b-88de-0e8f1c04813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample sentences - feel free to modify these!\n",
    "test_sentences = [\n",
    "    \"I traveled from New York to Richmond, Virginia last summer.\",\n",
    "    \"The battle took place near Harrisonburg in the Shenandoah Valley.\",\n",
    "    \"London and Paris are popular European destinations.\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    docs = geo.parse(test_sentences)\n",
    "    print(f\"‚úÖ Successfully parsed {len(docs)} sentences!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during parsing: {e}\")\n",
    "    print(\"Try restarting the kernel and running from the beginning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca949f-1d2b-43bf-9725-e4f7d40f030d",
   "metadata": {},
   "source": [
    "### Step 1.3: Examine the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327daaf9-f0de-4de6-9c4a-b86781d27355",
   "metadata": {},
   "source": [
    "Let's see what locations the geoparser found. Each \"toponym\" is a place name with detailed geographic information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03575a-c90c-46e3-a5e8-0691ae64ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è  LOCATIONS FOUND:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nSentence {i+1}: \\\"{test_sentences[i]}\\\"\")\n",
    "    \n",
    "    if doc.toponyms:\n",
    "        for toponym in doc.toponyms:\n",
    "            print(f\"  üìç Found: {toponym}\")\n",
    "    else:\n",
    "        print(\"  ‚ùå No locations found in this sentence\")\n",
    "        \n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dceb9cd-0f99-46e0-9135-f79660cb53e4",
   "metadata": {},
   "source": [
    "### Understanding the Data Structure\n",
    "\n",
    "Each toponym contains detailed geographic information. Here's what a complete location record looks like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'geonameid': 2867714,\n",
    "    'name': 'Munich',\n",
    "    'latitude': 48.13743,\n",
    "    'longitude': 11.57549,\n",
    "    'country_name': 'Germany',\n",
    "    'admin1_name': 'Bavaria',        # State/Province\n",
    "    'admin2_name': 'Upper Bavaria',  # County/Region\n",
    "    'feature_name': 'seat of a first-order administrative division',\n",
    "    'population': 1260391\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02179074-c249-40a7-871d-97e453508517",
   "metadata": {},
   "source": [
    "We can access specific pieces of information using `.location['key_name']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7291e59-eb5c-47c5-9a1f-7ac1d9c804c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific geographic information\n",
    "print(\"üìç DETAILED LOCATION DATA:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nSentence {i+1}: \\\"{test_sentences[i]}\\\"\")\n",
    "    \n",
    "    for toponym in doc.toponyms:\n",
    "        if toponym.location:\n",
    "            name = toponym.location['name']\n",
    "            lat = toponym.location['latitude']\n",
    "            lon = toponym.location['longitude']\n",
    "            country = toponym.location.get('country_name', 'Unknown')\n",
    "            \n",
    "            print(f\"  üèõÔ∏è  Place: {name}\")\n",
    "            print(f\"  üåç Country: {country}\")\n",
    "            print(f\"  üìê Coordinates: ({lat:.4f}, {lon:.4f})\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"  ‚ùå Location '{toponym}' could not be resolved to coordinates\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5a819-2928-4b91-a2b8-20aa858e5295",
   "metadata": {},
   "source": [
    "## Part 2: Load and Process Historical Text Data\n",
    "\n",
    "Now we'll work with real historical text that already has sentiment analysis completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082c609-1d80-494a-9335-f269cf96de51",
   "metadata": {},
   "source": [
    "### Step 2.1: Load Sentiment Data\n",
    "\n",
    "This dataset contains sentences from historical texts about Virginia, with sentiment scores already calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df00794-1c25-4f6e-9baa-a3c1617a5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_virginia_toponyms_sentiment = pd.read_pickle('df_virginia_toponym_sentiment_full.pickle')\n",
    "    print(f\"‚úÖ Loaded {len(df_virginia_toponyms_sentiment):,} sentences with sentiment data\")\n",
    "    print(f\"üìä Columns: {list(df_virginia_toponyms_sentiment.columns)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data file not found!\")\n",
    "    print(\"You may need to run previous lessons first to generate the sentiment data.\")\n",
    "    print(\"Or check that you're in the correct directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc124384-68d5-4bf3-a15e-8ca175e8f98a",
   "metadata": {},
   "source": [
    "### Step 2.2: Demo with Sample Data\n",
    "\n",
    "**‚è±Ô∏è Note**: Full geoparsing takes 1+ hours, so we'll demonstrate with a small sample first, then load pre-processed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750fadb3-bed7-4ed6-a624-b77ba464a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small sample for demonstration\n",
    "try:\n",
    "    df_virginia_sample = df_virginia_toponyms_sentiment.head(100).copy()\n",
    "    print(f\"üìù Created sample with {len(df_virginia_sample)} sentences\")\n",
    "    print(\"\\nüîç Sample of the data:\")\n",
    "    display(df_virginia_sample[['cleaned_sentences', 'roberta_pos', 'roberta_neu', 'roberta_neg']].head(3))\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå Data not loaded yet. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79209b12-06db-4fb7-8fc0-bd35d07cc5fd",
   "metadata": {},
   "source": [
    "### Step 2.3: The Geoparsing Function\n",
    "\n",
    "Here's a streamlined function that processes text and extracts geographic information:\n",
    "\n",
    "**Key features:**\n",
    "- Processes multiple sentences at once for efficiency\n",
    "- Filters for Administrative areas (Countries, States) and Population centers (Cities)\n",
    "- Extracts coordinates and place information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fef83-5a3a-4783-8b78-6e2b04d64338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoparse_dataframe(df, text_column='cleaned_sentences'):\n",
    "    \"\"\"\n",
    "    Extract geographic locations from text data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text data\n",
    "        text_column: Column containing the text to parse\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added location columns\n",
    "    \"\"\"\n",
    "    print(f\"üîç Processing {len(df)} sentences for geographic locations...\")\n",
    "    \n",
    "    # Convert text column to list for batch processing\n",
    "    sentences = df[text_column].tolist()\n",
    "    \n",
    "    try:\n",
    "        # Process all sentences at once (more efficient)\n",
    "        docs = geo.parse(sentences, feature_filter=['A', 'P'])  # A=Administrative, P=Population centers\n",
    "        \n",
    "        # Initialize storage for results\n",
    "        places, latitudes, longitudes, feature_names = [], [], [], []\n",
    "        \n",
    "        # Extract information from each processed document\n",
    "        for doc in tqdm(docs, desc=\"Extracting locations\"):\n",
    "            doc_places = []\n",
    "            doc_latitudes = []\n",
    "            doc_longitudes = []\n",
    "            doc_feature_names = []\n",
    "            \n",
    "            # Get all toponyms found in this document\n",
    "            for toponym in doc.toponyms:\n",
    "                if toponym.location:\n",
    "                    doc_places.append(toponym.location.get('name'))\n",
    "                    doc_latitudes.append(toponym.location.get('latitude'))\n",
    "                    doc_longitudes.append(toponym.location.get('longitude'))\n",
    "                    doc_feature_names.append(toponym.location.get('feature_name'))\n",
    "            \n",
    "            # Store results (empty lists if no locations found)\n",
    "            places.append(doc_places)\n",
    "            latitudes.append(doc_latitudes)\n",
    "            longitudes.append(doc_longitudes)\n",
    "            feature_names.append(doc_feature_names)\n",
    "        \n",
    "        # Add new columns to dataframe\n",
    "        df_result = df.copy()\n",
    "        df_result['place'] = places\n",
    "        df_result['latitude'] = latitudes\n",
    "        df_result['longitude'] = longitudes\n",
    "        df_result['feature_name'] = feature_names\n",
    "        \n",
    "        print(f\"‚úÖ Geoparsing complete!\")\n",
    "        return df_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during geoparsing: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d730b30-80e7-4f01-81f6-6bd443b0435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run geoparsing on sample data\n",
    "try:\n",
    "    geoparse_results_sample = geoparse_dataframe(df_virginia_sample)\n",
    "    print(f\"\\nüìä Results: {len(geoparse_results_sample)} sentences processed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure the geoparser was initialized successfully in Part 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84ab6e-2652-479e-aaf3-b3c71dbf36bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geoparse_results_sample.sample(5, random_state =10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e802f-76ab-49eb-88dd-ab910b19b5f6",
   "metadata": {},
   "source": [
    "There are several interesting things of note in the data. First, for some of the sentences the tokenizer did not find a toponym which is indicated by empty lists `[]`. This because this is a more accurate tokenizer and will likely have fewer false positives. We will have to remember to remove these. Likewise, right now the parsing has been set to include Administrative areas like countries and states (i.e. The US and Virginia) and population centers (Richmond, Harrisonburg). We will have to think of how to deal with these down the road.\n",
    "\n",
    "**We can run the geoparser for all the data and expect to wait at least an hour!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a547180-0c60-4bf2-b0eb-22e9a2e8b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the geoparser over the entire 'cleaned_sentences' column\n",
    "#geoparse_results = geoparse_column(df_virginia_toponyms_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70a802-158f-4988-bf66-9b52aa048451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated DataFrame with new columns\n",
    "#geoparse_results.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca69a7e-18f7-4543-8721-87454a0bf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geoparse_results.to_pickle('df_virginia_geoparsed_complete.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022afd6-59c8-4a67-baf4-a80604806e4e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812773e-e1cc-4249-a4de-6a6819707331",
   "metadata": {},
   "source": [
    "## Part 3: Work with Complete Geoparsed Dataset\n",
    "\n",
    "Since geoparsing the full dataset takes hours, we'll load the pre-processed results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6066a5-23cd-480d-a71f-2f2309e70566",
   "metadata": {},
   "source": [
    "### Step 3.1: Load Pre-processed Data\n",
    "\n",
    "The complete geoparsing process took over an hour on a modern computer:\n",
    "\n",
    "![Processing Time](geoparser_completion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f1ba0-0275-47c4-a7e4-9c9711db6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_virginia_all = pd.read_pickle('df_virginia_geoparsed_complete.pickle')\n",
    "    print(f\"‚úÖ Loaded complete dataset: {len(df_virginia_all):,} sentences\")\n",
    "    print(f\"üìä New columns added: place, latitude, longitude, feature_name\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Complete geoparsed file not found!\")\n",
    "    print(\"This file should be provided with the lesson materials.\")\n",
    "    print(\"You can also create it by running the full geoparsing process (takes 1+ hours).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f75ced-21a1-4a71-b39f-08dfca3d7c43",
   "metadata": {},
   "source": [
    "### Step 3.2: Understanding Accuracy\n",
    "\n",
    "The advanced geoparser is more accurate than simple named entity recognition, so it produces fewer false positives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b2b58-2970-4f91-b818-1ee51c1b0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_all[['toponyms','place']].sample(7, random_state = 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb18438-136a-4972-afdc-46c940dde009",
   "metadata": {},
   "source": [
    "Calculate the number of false positives in the original toponyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54452e2-13f0-4e7c-83b5-6f1270850f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_percent = (df_virginia_all['place'].str.len() == 0).mean() * 100\n",
    "print(f\"The number of missing values is {empty_percent:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943e607-0acf-4e7e-8ae8-34448cd9a75b",
   "metadata": {},
   "source": [
    "### 2.1 Remove false positives\n",
    "\n",
    "Here a false positive is defined as any empty value in the more fine-grained value in the toponym parser and a hit in the rough NER extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fba70-b9a9-4ad2-ba3f-39882e7bb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_cleaned = df_virginia_all[df_virginia_all['place'].str.len() != 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc6fc0-c774-4d08-9674-bbb701d59569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_cleaned[['cleaned_sentences','place','latitude','longitude']].sample(5, random_state= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b66ab3-3e3a-40a2-80bb-94f23dbe9b6d",
   "metadata": {},
   "source": [
    "#### 2.1.1 `explode` the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ccdce-449b-450c-8b66-4dd34a0046a3",
   "metadata": {},
   "source": [
    "As previously, we want to get all of the data per row. Some sentences contained multiple locations so we want to get these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168914f-b978-4efa-ac4a-5801a6fa7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_long = df_virginia_cleaned.explode(['place', 'latitude', 'longitude', 'feature_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfebf841-e6e2-46ad-acdf-9208fec86021",
   "metadata": {},
   "source": [
    "#### 2.1.2 Investigate the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591cb86d-14a9-42d5-9833-b2557f847794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_long[['place', 'latitude', 'longitude', 'feature_name']].sample(5, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0707b-150a-4c03-907a-a6541f6b4ec1",
   "metadata": {},
   "source": [
    "#### 2.1.3 Critical Question\n",
    "\n",
    "From a cartographic perspective, what is the difference between the types of features recovered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15baa5e1-e500-444b-8c89-1281b09f2c9e",
   "metadata": {},
   "source": [
    "### 2.2 Remove empty values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708913f1-b2d8-4b89-a211-50ccb104b45e",
   "metadata": {},
   "source": [
    "We can remove all of the `None` values with `.notna()`. That is, if the value is not `None` return true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2412b-01eb-4646-8360-3c9ef901e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_long = df_virginia_long[df_virginia_long.place.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440fe40-07ef-484f-9cfa-12b6163141a6",
   "metadata": {},
   "source": [
    "When we do this we have to remember to reset the index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af91934-4d13-4ebf-9477-276b74998e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_long = df_virginia_long.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147fddec-a8e1-414b-ac2a-2aa46d0f640d",
   "metadata": {},
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834edbd9-d434-4bf1-b873-c085a5ca35a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_virginia_long.sample(3, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3bbfce-3705-4db8-b091-56aa009a81ec",
   "metadata": {},
   "source": [
    "### 2.3 Creating an Aggregate table with `.groupby()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd155852-ef3a-4613-9ea4-f4d0953a8f81",
   "metadata": {},
   "source": [
    "We now have a list of every place, latitude, longitude, and type of place it is. This is coupled with the sentiment data. We now need to consolidate this into singular points. We are going to do this by using the `.groupby()` method, which makes the calculation by group. In this case our group is going to be `place`, meaning that for every place it will count the number of times it appears and also the average sentiment score. We group by `place` to consolidate multiple mentions of the same location and calculate the average sentiment to get a general sentiment trend for each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671432fe-c9df-4fd6-8d97-57801b837f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations_sentiments = df_virginia_long.groupby('place').agg(\n",
    "    location_count=('place', 'size'),  # Count occurrences of each location\n",
    "    latitude=('latitude', 'first'),    # Take the first latitude (you can also use 'mean')\n",
    "    longitude=('longitude', 'first'),  # Take the first longitude (or 'mean')\n",
    "    location=('feature_name','first'),\n",
    "    avg_roberta_pos=('roberta_pos', 'mean'),  # Average of roberta_pos\n",
    "    avg_roberta_neu=('roberta_neu', 'mean'),  # Average of roberta_neu\n",
    "    avg_roberta_neg=('roberta_neg', 'mean')   # Average of roberta_neg\n",
    ").reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01d085-a46d-4c48-abfb-c2b3baeb69eb",
   "metadata": {},
   "source": [
    "Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4cf27-b21f-43a7-bb33-36ee8810bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations_sentiments.sample(5, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ab743-3442-42ac-87c1-93f612ba2ca6",
   "metadata": {},
   "source": [
    "#### 2.3.1 Find top locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0606b-8cdd-4453-9a48-3441dfe86e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations_sentiments.sort_values(by='location_count', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d06c0-f0f9-49df-a648-04bbc0672e52",
   "metadata": {},
   "source": [
    "What can we tell about the number of frequent locations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f72c0-2c93-4012-b3bc-4b84c3a1fd81",
   "metadata": {},
   "source": [
    "### 2.4 Consolidate the Roberta Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b35ed0-4f5f-4a0e-8339-d9e44dd8a7e1",
   "metadata": {},
   "source": [
    "Since the roberta score is positive, negative, and neutral, we will have to consolidate it into one easier to understand score. We will take the difference between positive and negative, and multiply it by the percentage of neutral. This way if a score is very neutral it will even out the difference between positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e44f8-542d-4b7b-b290-97fa6b0bc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the compound score and add it as a new column 'roberta_compound'\n",
    "df_geolocations_sentiments['avg_roberta_compound'] = (\n",
    "    df_geolocations_sentiments['avg_roberta_pos'] - df_geolocations_sentiments['avg_roberta_neg']\n",
    ") * (1 - df_geolocations_sentiments['avg_roberta_neu'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c623b-ba0c-4b43-ae00-4a4d42130f98",
   "metadata": {},
   "source": [
    "#### 2.4.1 The most negative place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e83f9-6fe3-47b8-bcad-c31e0912f553",
   "metadata": {},
   "source": [
    "Let's find the most negative place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3e8ba-32b0-4691-8f2e-0c699c0d396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'roberta_compound' in ascending order and display the top 10 negative scores\n",
    "top_10_negative = df_geolocations_sentiments.sort_values(by='avg_roberta_compound').head(10)\n",
    "top_10_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611334a9-fd30-41e2-99db-f78809534d7b",
   "metadata": {},
   "source": [
    "### 2.4.2 Critical Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4471b8b-b1db-40d6-82bc-4c465448c363",
   "metadata": {},
   "source": [
    "We can already tell there might be some challenges here. The values with the strongest scores tend to be low in count. \n",
    "\n",
    "- Why might this be?\n",
    "\n",
    "Also, these are different types of features. \n",
    "\n",
    "- How might this distort results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83891dfa-0385-4133-a1eb-91b6024a4ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_geolocations_sentiments.to_pickle('df_geolocations_sentiments.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b4330-46af-41e6-9126-ba2aaad831d0",
   "metadata": {},
   "source": [
    "##### 3 (Optional) Load in completed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed3b99-e509-44b6-b29e-b1c266928e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations_sentiments = pd.read_pickle('df_geolocations_sentiments.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7edefa-257f-44cd-bcf5-81f50e89441b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35650fb-e2f3-4353-a53d-4e384a854e00",
   "metadata": {},
   "source": [
    "## Part 4: Create Interactive Sentiment Maps\n",
    "\n",
    "Now we'll create beautiful interactive maps showing how sentiment varies by location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6245eaf-e9d3-42bb-8485-b2ebf1290a13",
   "metadata": {},
   "source": [
    "### 3.1 Basic Plotly Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f455d9-07b0-4563-9558-392c4bf5bfe6",
   "metadata": {},
   "source": [
    "Below is the basic stub from the instructions to create a [map](https://plotly.com/python/tile-scatter-maps/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf42836-2e28-48cc-b384-9ee7f463b404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create the map using plotly.express \n",
    "fig = px.scatter_mapbox(\n",
    "    df_geolocations_sentiments,  #put your dataframe here\n",
    "    lat=\"latitude\",               # Latitude column\n",
    "    lon=\"longitude\",              # Longitude column\n",
    "    size=\"location_count\",        # Bubble size based on location count\n",
    "    color=\"avg_roberta_compound\",      # Color based on sentiment score\n",
    "    color_continuous_scale=px.colors.cyclical.IceFire,  # Use IceFire scale (blue to red)\n",
    "    size_max=15,                  # Maximum size of the bubbles\n",
    "    center={\"lat\": 0.0, \"lon\": 0.0},\n",
    "    zoom=1                        # Adjust zoom level for better visibility\n",
    ")\n",
    "\n",
    "# Update the layout to use the default map style (which doesn't need a token)\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"open-street-map\",  # No token needed for this style\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}  # Remove margins for a cleaner view\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3966c3-ac66-4664-9a18-8f458842897a",
   "metadata": {},
   "source": [
    "### 3.1 Critical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710168bb-e3dc-4415-a358-a66e79693ab0",
   "metadata": {},
   "source": [
    "- What are some issue with this map?\n",
    "- How can we make it better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca7944-aedd-43c1-b788-4108f6aeb889",
   "metadata": {},
   "source": [
    "#### 3.2.1 Adjust the position and the zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b81f6-cd4a-4bd8-9d80-a98a9193dd17",
   "metadata": {},
   "source": [
    "Look at the comments next to each variable in the plotly function, what does each thing do? \n",
    "- If I wanted to get a closer zoom how would I fix it?\n",
    "- If I want to set a different center what should I choose?\n",
    "- How would I change the size of the bubbles?\n",
    "- How can I get a different mapbox style?\n",
    "\n",
    "Take some time to mess around with your map.\n",
    "\n",
    "\n",
    "[Read the full documentation](https://plotly.github.io/plotly.py-docs/generated/plotly.express.scatter_mapbox.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ef1e7-4b21-49bb-95f5-49b6a06ab9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the map using plotly.express \n",
    "fig = px.scatter_mapbox(\n",
    "    df_geolocations_sentiments,  #put your dataframe here\n",
    "    lat=\"latitude\",               # Latitude column\n",
    "    lon=\"longitude\",              # Longitude column\n",
    "    size=\"location_count\",        # Bubble size based on location count\n",
    "    color=\"avg_roberta_compound\",      # Color based on sentiment score\n",
    "    color_continuous_scale=px.colors.cyclical.IceFire,  # Use IceFire scale (blue to red)\n",
    "    size_max=50,                  # Maximum size of the bubbles\n",
    "    center={\"lat\": 0.0, \"lon\": 0.0},\n",
    "    zoom=1                        # Adjust zoom level for better visibility\n",
    ")\n",
    "\n",
    "# Update the layout to use the default map style (which doesn't need a token)\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"open-street-map\",  # No token needed for this style\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}  # Remove margins for a cleaner view\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fa587-1f53-4711-bffc-5a2f1a312026",
   "metadata": {},
   "source": [
    "### 3.3 Bubble Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf413b-7d8c-44e4-9dc2-90ae5a5bcf40",
   "metadata": {},
   "source": [
    "One major issue is simply the size of the bubbles. The data is very spread out. The lowest number is 1 and the highest number is 11000. We can make life a little easier by simply removing some of the lower numbers. We can do this randomly by simply removing every below a certain count or we can be a bit more thoughtful and only consider a certain percentage of values. We can actually get a nice summary of a column using the `.describe()` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c1380-aee4-479b-93a4-b232441d5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations_sentiments.location_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b46327-8370-475f-b099-ef3ccf7849e7",
   "metadata": {},
   "source": [
    "It actually looks like a significant number of locations appear very few times. We will have to keep this in mind going forward. For now, let's set our count threshold to 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5bcd83-9617-4dc6-b692-a95166cfc339",
   "metadata": {},
   "source": [
    "### 3.4 Critical Question\n",
    "\n",
    "Create a new dataframe that sets the minimum location count to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711bd83-2d6a-46be-8d5b-87140f1fb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations_sentiments_small = df_geolocations_sentiments[df_geolocations_sentiments.location_count>99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419c134-d456-4a55-8cd8-04473ddab8a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the map using plotly.express \n",
    "fig = px.scatter_mapbox(\n",
    "    df_geolocations_sentiments_small,  #put your dataframe here\n",
    "    lat=\"latitude\",               # Latitude column\n",
    "    lon=\"longitude\",              # Longitude column\n",
    "    size=\"location_count\",        # Bubble size based on location count\n",
    "    color=\"avg_roberta_compound\",      # Color based on sentiment score\n",
    "    color_continuous_scale=px.colors.cyclical.IceFire,  # Use IceFire scale (blue to red)\n",
    "    size_max=20,                  # Maximum size of the bubbles\n",
    "    center={\"lat\": 37.5246322, \"lon\": -77.5758331},\n",
    "    zoom=4                        # Adjust zoom level for better visibility\n",
    ")\n",
    "\n",
    "# Update the layout to use the default map style (which doesn't need a token)\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"carto-darkmatter\",  # No token needed for this style\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}  # Remove margins for a cleaner view\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9189aac-9b80-4ea1-88b6-078f430abdf4",
   "metadata": {},
   "source": [
    "### 3.5 Critical Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4874d-6fe9-4127-852e-27a81185f981",
   "metadata": {},
   "source": [
    "- How did reducing the number of values improve the data visualization?\n",
    "- Where does it run into issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1906cc-a117-4c6f-b131-4d0706295529",
   "metadata": {},
   "source": [
    "### 3.6 Bucketing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a46ffd-177a-495e-9cd3-e99f6e93c777",
   "metadata": {},
   "source": [
    "The above map still runs into a sizing issue. This is because even though the values are a bit closer together there is still a difference of 11,000 between the lowest value and the highest value. This is a common problem when representing values on a map. The easiest way to fix this is to \"bucket\" the values from \"smallest\" to \"small\" \"medium\", \"large\", and \"largest\". The problem is deciding how to do this in the most optimal way. \n",
    "\n",
    "   - **Equal Interval**: This method divides the entire range of values into equal-sized buckets. It‚Äôs simple to implement and works well when data is evenly distributed. However, it may not be as effective when the data is heavily skewed, as it can lead to many data points clustering within certain buckets.\n",
    "   - **Quantile (Percentiles)**: Quantile bucketing divides data so that each bucket contains an equal number of data points. This method is useful for data with uneven distributions, as it ensures that each category has a similar representation.\n",
    "   - **Natural Breaks (Jenks)**: The Jenks method automatically identifies clusters and gaps within the data to create buckets based on natural groupings. This technique is particularly beneficial for data with distinct groupings, as it helps to highlight these patterns and produce visually distinct buckets that better reflect the distribution of values.\n",
    "\n",
    "\n",
    "We are going to go with natural breaks using `mapclassify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fcb52-7f45-4143-8ec5-75bbfb750755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mapclassify as mc #you may get an error. If so install mapclassify with pip install mapclassify\n",
    "\n",
    "jenks_breaks = mc.NaturalBreaks(y=df_geolocations_sentiments_small['location_count'], k=5)\n",
    "jenks_breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b7770-a443-45dc-9d31-4d676fa4d987",
   "metadata": {},
   "source": [
    "#### 3.6.1 Critical Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb4c2d-7c29-43e8-8172-5ab398a8b035",
   "metadata": {},
   "source": [
    "Now that we have a better view of the data, what would it have looked like with equal interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949cab4-923b-4aa7-8971-4740ff21f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'location_count_bucket' for the classified values\n",
    "df_geolocations_sentiments_small.loc[:,'location_count_bucket'] = jenks_breaks.find_bin(df_geolocations_sentiments_small['location_count'])+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980d899-2a6c-4f95-9528-f29aedab25b5",
   "metadata": {},
   "source": [
    "#### 3.6.2 Optional Retrieve Backup\n",
    "\n",
    "get copy of `df_geolocations_sentiments_small.pickle` if the above does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bf562-48a3-4a89-81e9-c1c7487c832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_geolocations_sentiments_small = pd.read_pickle(\"df_geolocations_sentiments_small.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e913ee3-eddd-4af6-a05e-de35cd76f7e9",
   "metadata": {},
   "source": [
    "#### 3.6.3 Explore Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fa951-f1a0-424a-8254-00f03165d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations_sentiments_small.sample(5, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805b235-ba65-4c97-a45b-e5b898a9deb7",
   "metadata": {},
   "source": [
    "What do we notice about the buckets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c8616-0a9e-4045-96c5-26e8b515fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the map using plotly.express \n",
    "fig = px.scatter_mapbox(\n",
    "    df_geolocations_sentiments_small,  #put your dataframe here\n",
    "    lat=\"latitude\",               # Latitude column\n",
    "    lon=\"longitude\",              # Longitude column\n",
    "    size=\"location_count_bucket\",        # Bubble size based on location count\n",
    "    color=\"avg_roberta_compound\",      # Color based on sentiment score\n",
    "    color_continuous_scale=px.colors.cyclical.IceFire,  # Use IceFire scale (blue to red)\n",
    "    size_max=20,                  # Maximum size of the bubbles\n",
    "    center={\"lat\": 37.5246322, \"lon\": -77.5758331},\n",
    "    zoom=4                        # Adjust zoom level for better visibility\n",
    ")\n",
    "\n",
    "# Update the layout to use the default map style (which doesn't need a token)\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"carto-darkmatter\",  # No token needed for this style\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}  # Remove margins for a cleaner view\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac316c-1b0f-4039-9cff-2c51dd2b4423",
   "metadata": {},
   "source": [
    "### 3.7 Improving labels and colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7262d-5ee2-440f-906f-bae334120b61",
   "metadata": {},
   "source": [
    "Now the markers need some labels and the colors need to be reversed and need a new midpoint.\n",
    "\n",
    "We can set the midpoint for the color scale with the argument:\n",
    "\n",
    "```python\n",
    "color_continuous_scale=0,\n",
    "```\n",
    "\n",
    "We can reverse the `IceFire` color scale using reverse list slicing \n",
    "\n",
    "```python\n",
    "[::-1]\n",
    "```\n",
    "\n",
    "Or you can pick a different one [here](https://plotly.com/python/builtin-colorscales/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "909d65a1-64c6-4a13-b6d4-cec9ac038275",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_geolocations_sentiments_small' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m      3\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mscatter_mapbox(\n\u001b[1;32m----> 4\u001b[0m     df_geolocations_sentiments_small,  \u001b[38;5;66;03m# DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     lat\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m,                   \u001b[38;5;66;03m# Latitude column\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     lon\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m,                  \u001b[38;5;66;03m# Longitude column\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation_count_bucket\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \u001b[38;5;66;03m# Bubble size based on location count bucket\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_roberta_compound\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \u001b[38;5;66;03m# Color based on sentiment score\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     color_continuous_scale\u001b[38;5;241m=\u001b[39mpx\u001b[38;5;241m.\u001b[39mcolors\u001b[38;5;241m.\u001b[39mcyclical\u001b[38;5;241m.\u001b[39mEdge[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# IceFire color scale\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     color_continuous_midpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,      \u001b[38;5;66;03m# Set 0 as the center point\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     size_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,                      \u001b[38;5;66;03m# Maximum size of the bubbles\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     center\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m37.5246322\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m77.5758331\u001b[39m},\n\u001b[0;32m     13\u001b[0m     zoom\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,                           \u001b[38;5;66;03m# Zoom level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     hover_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplace\u001b[39m\u001b[38;5;124m\"\u001b[39m,               \u001b[38;5;66;03m# Show place name\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     hover_data\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     16\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_roberta_compound\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:.2f\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;66;03m# Rounded sentiment score to 2 decimals\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation_count_bucket\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     }\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Update the layout to use the default map style\u001b[39;00m\n\u001b[0;32m     24\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(\n\u001b[0;32m     25\u001b[0m     mapbox_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcarto-darkmatter\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# No token needed\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     margin\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}  \u001b[38;5;66;03m# Remove margins for cleaner view\u001b[39;00m\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_geolocations_sentiments_small' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    df_geolocations_sentiments_small,  # DataFrame\n",
    "    lat=\"latitude\",                   # Latitude column\n",
    "    lon=\"longitude\",                  # Longitude column\n",
    "    size=\"location_count_bucket\",     # Bubble size based on location count bucket\n",
    "    color=\"avg_roberta_compound\",     # Color based on sentiment score\n",
    "    color_continuous_scale=px.colors.cyclical.Edge[::-1],  # IceFire color scale\n",
    "    color_continuous_midpoint=0,      # Set 0 as the center point\n",
    "    size_max=20,                      # Maximum size of the bubbles\n",
    "    center={\"lat\": 37.5246322, \"lon\": -77.5758331},\n",
    "    zoom=4,                           # Zoom level\n",
    "    hover_name=\"place\",               # Show place name\n",
    "    hover_data={\n",
    "            \"avg_roberta_compound\": ':.2f',   # Rounded sentiment score to 2 decimals\n",
    "        \"longitude\": False,\n",
    "        \"latitude\": False,\n",
    "        \"location_count_bucket\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# Update the layout to use the default map style\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"carto-darkmatter\",  # No token needed\n",
    "    margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0}  # Remove margins for cleaner view\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c0a3a-e9f0-412c-bc9f-45b171199c5e",
   "metadata": {},
   "source": [
    "How else can this map be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2751a39-4f94-435b-9b02-bc3edc151f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations_sentiments_small.to_pickle('df_geolocations_sentiments_small.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19680603",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Lesson Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. **üîß Setup & Installation**: Created a one-stop installation notebook for all dependencies\n",
    "2. **üó∫Ô∏è Geoparsing**: Used advanced AI to extract and resolve geographic locations from text\n",
    "3. **üìä Data Integration**: Combined location data with sentiment analysis results\n",
    "4. **üé® Interactive Mapping**: Created beautiful, interactive maps showing sentiment patterns\n",
    "\n",
    "### Key Skills Learned:\n",
    "\n",
    "- **Advanced Text Processing**: Using transformer models for geographic entity recognition\n",
    "- **Data Pipeline**: Combining multiple data processing steps into a coherent workflow  \n",
    "- **Geographic Visualization**: Creating sophisticated interactive maps with plotly\n",
    "- **Data Science**: Handling real-world data challenges like false positives and scale differences\n",
    "\n",
    "### Technical Tools Mastered:\n",
    "\n",
    "- `geoparser`: State-of-the-art geographic text processing\n",
    "- `plotly`: Interactive data visualization  \n",
    "- `mapclassify`: Intelligent data bucketing for visualization\n",
    "- `pandas`: Advanced data manipulation and aggregation\n",
    "\n",
    "### Applications:\n",
    "\n",
    "This workflow can be applied to:\n",
    "- **Literary Analysis**: Studying geographic patterns in literature\n",
    "- **Historical Research**: Mapping sentiment about places over time\n",
    "- **Social Media Analysis**: Understanding geographic sentiment in tweets/posts\n",
    "- **News Analysis**: Tracking how different locations are portrayed in media\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try this workflow with your own text data\n",
    "- Experiment with different visualization styles and color schemes\n",
    "- Explore temporal patterns (how sentiment changes over time)\n",
    "- Combine with other data sources (demographics, economics, etc.)\n",
    "\n",
    "üéâ **Congratulations!** You've completed a full geospatial text analysis pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
